# This config contains the default values for training Flowtron model on LJSpeech dataset.
# If you want to train model on other dataset, you can change config values according to your dataset.
# Most dataset-specific arguments are in the head of the config file, see below.

name: Flowtron

# paths to datasets
train_dataset: ???
validation_dataset: ???

model:
  dummy_speaker_embedding: false
  n_flows: 2
  n_components: 0
  use_gate_layer: true
  n_hidden: 640
  n_attn_channels: 1024
  n_lstm_layers: 2
  use_cumm_attention: false
  n_mel_channels: 80
  n_speaker_dim: 128
  n_text_dim: 512
  seed: 1234
  fp16_run: true
  use_ctc_loss: true
  
  speakeremb:
    n_speakers: 1
    n_speaker_dim: ${model.n_speaker_dim}

  textemb:
    n_text: 185
    n_text_dim: ${model.n_text_dim}

  encoder:
    _target_: nemo.collections.tts.modules.flowtron_modules.Encoder
    encoder_n_convolutions: 3
    encoder_embedding_dim: ${model.n_text_dim}
    encoder_kernel_size: 5
    norm_fn: torch.nn.InstanceNorm1d

  melencoder:
    _target_: nemo.collections.tts.modules.flowtron_modules.MelEncoder
    encoder_n_convolutions: 2
    encoder_embedding_dim: 512
    encoder_kernel_size: 3
    norm_fn: torch.nn.InstanceNorm1d

  gaussianmixture:
    _target_: nemo.collections.tts.modules.flowtron_modules.GaussianMixture
    n_hidden: ${model.melencoder.encoder_embedding_dim}
    n_components: ${model.n_components}
    n_mel_channels: ${model.n_mel_channels}
    fixed_gaussian: true
    mean_scale: 0.0

  train_ds:
    dataset:
      _target_: "nemo.collections.tts.data.datalayers.FlowtronData"
      manifest_filepath: ${train_dataset}
      filter_length: 1024
      hop_length: 256
      win_length: 1024
      sampling_rate: 22050
      mel_fmin: 0.0
      mel_fmax: 8000.0
      max_wav_value: 32768.0
      p_arpabet: 0.5
      cmudict_path: "data/cmudict_dictionary"
      text_cleaners: ["flowtron_cleaners"]
      speaker_ids: null
      use_attn_prior: false
      attn_prior_threshold: 0.0
      prior_cache_path: "/attention_prior_cache"
      betab_scaling_factor: 1.0
      keep_ambiguous: false 
      seed: ${trainer.seed}
      randomize: true
    dataloader_params:
      num_workers: 1
      shuffle: true
      sampler: null 
      batch_size: ${trainer.batch_size}
      pin_memory: false
      drop_last: true
  
  validation_ds:
    dataset: ${model.train_ds.dataset}
    dataloader_params:
      num_workers: 1
      shuffle: false
      sampler: null 
      batch_size: ${trainer.batch_size}
      pin_memory: false

trainer:
  n_gpus: 1
  rank: 0
  output_directory: "/outdir"
  epochs: 10000000
  optim_algo: "RAdam"
  learning_rate: 1e-3
  weight_decay: 1e-6
  sigma: 1.0
  iters_per_checkpoint: 1000
  batch_size: 6
  checkpoint_path: ""
  ignore_layers: []
  include_layers: ["speaker", "encoder", "embedding"]
  finetune_layers: []
  warmstart_checkpoint_path: ""
  with_tensorboard: true
  grad_clip_val: 1
  gate_loss: true
  ctc_loss_weight: 0.01
  blank_logprob: -8
  ctc_loss_start_iter: 10000