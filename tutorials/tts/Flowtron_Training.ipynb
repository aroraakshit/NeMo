{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45674b40",
   "metadata": {},
   "source": [
    "# Flowtron Training\n",
    "\n",
    "This notebook covers:\n",
    "1. Generate spectrogram from a pre-trained model on NGC & or locally available\n",
    "2. Training Flowtron with LJSpeech dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116842a",
   "metadata": {},
   "source": [
    "## 1. Generate spectrogram from a pre-trained model on NGC or local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6816746",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119dbbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c39d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040b2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b6e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a369c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15631623",
   "metadata": {},
   "source": [
    "## 2. Training Flowtron with LJSpeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f11006",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH = 'main'\n",
    "\n",
    "!wget https://raw.githubusercontent.com/aroraakshit/NeMo/$BRANCH/examples/tts/flowtron.py\n",
    "!mkdir conf && cd conf && wget https://raw.githubusercontent.com/aroraakshit/NeMo/$BRANCH/examples/tts/conf/flowtron.yaml && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c90785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading LJS dataset\n",
    "# !mkdir data && cd data && wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2 && tar -xvjf archive.tar.bz2 && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db560e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run to test flowtron\n",
    "\n",
    "!PYTHONPATH=/akshita/NeMo CUDA_VISIBLE_DEVICES=2 HYDRA_FULL_ERROR=1 python flowtron.py train_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_100train_filelist.txt\" validation_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_10val_filelist.txt\" trainer.max_epochs=5 trainer.flush_logs_every_n_steps=10 trainer.val_check_interval=20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155191c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train using the attention prior and the alignment loss (CTC loss) until attention looks good\n",
    "\n",
    "!PYTHONPATH=/akshita/NeMo CUDA_VISIBLE_DEVICES=2,1 python flowtron.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# components > 0 on a sample run\n",
    "\n",
    "#!PYTHONPATH=/akshita/NeMo HYDRA_FULL_ERROR=1 python flowtron.py train_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_100train_filelist.txt\" validation_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_10val_filelist.txt\" ++model.n_components=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837486b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-02-11 23:28:00 experimental:27] Module <function get_argmin_mat at 0x7f1c89cc28b0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-11 23:28:00 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x7f1c89cc2940> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-11 23:28:00 experimental:27] Module <function parse_scale_configs at 0x7f1c89ccc3a0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-11 23:28:00 experimental:27] Module <function get_embs_and_timestamps at 0x7f1c89ccc430> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-11 23:28:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-02-11 23:28:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:45: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v1.7. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-02-11 23:28:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-02-11 23:28:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:59: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=10)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2022-02-11 23:28:01 exp_manager:283] Experiments will be logged at /akshita/NeMo/tutorials/tts/nemo_experiments/Flowtron/2022-02-11_23-28-01\n",
      "[NeMo I 2022-02-11 23:28:01 exp_manager:648] TensorboardLogger has been set up\n",
      "[NeMo W 2022-02-11 23:28:01 exp_manager:901] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2022-02-11 23:28:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "[NeMo W 2022-02-11 23:28:04 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1906: LightningDeprecationWarning: `trainer.resume_from_checkpoint` is deprecated in v1.5 and will be removed in v1.7. Specify the fit checkpoint path with `trainer.fit(ckpt_path=)` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-02-11 23:28:04 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-02-11 23:28:04 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Restoring states from the checkpoint path at /akshita/NeMo/tutorials/tts/nemo_experiments/Flowtron/Flowtron--val_loss=-0.8421-epoch=49.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "Initializing RAdam optimizer\n",
      "[NeMo W 2022-02-11 23:28:08 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:247: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Restored all states from the checkpoint file at /akshita/NeMo/tutorials/tts/nemo_experiments/Flowtron/Flowtron--val_loss=-0.8421-epoch=49.ckpt\n",
      "\n",
      "  | Name              | Type         | Params\n",
      "---------------------------------------------------\n",
      "0 | speaker_embedding | Embedding    | 128   \n",
      "1 | embedding         | Embedding    | 94.7 K\n",
      "2 | flows             | ModuleList   | 55.4 M\n",
      "3 | encoder           | Encoder      | 5.5 M \n",
      "4 | criterion         | FlowtronLoss | 0     \n",
      "---------------------------------------------------\n",
      "61.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.0 M    Total params\n",
      "243.910   Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s][NeMo W 2022-02-11 23:28:08 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2022-02-11 23:28:18 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2022-02-11 23:28:18 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:407: UserWarning: The number of training samples (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 50:   0%|                                          | 0/50 [00:00<?, ?it/s][W reducer.cpp:1296] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[NeMo W 2022-02-11 23:28:24 nemo_logging:349] /akshita/NeMo/nemo/collections/tts/modules/flowtron_submodules.py:779: UserWarning: This overload of addcmul_ is deprecated:\n",
      "    \taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "    Consider using one of the following signatures instead:\n",
      "    \taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "    \n",
      "Epoch 50:  14%|█▎       | 7/50 [00:21<02:09,  3.02s/it, loss=-0.825, v_num=8-01]"
     ]
    }
   ],
   "source": [
    "# sample Step 2: Resume training without the attention prior once the alignments have stabilized\n",
    "\n",
    "!PYTHONPATH=/akshita/NeMo CUDA_VISIBLE_DEVICES=2 HYDRA_FULL_ERROR=1 python flowtron.py --config-path=\"conf\" --config-name=\"flowtron_s2\" train_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_100train_filelist.txt\" validation_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_10val_filelist.txt\" trainer.max_epochs=100 trainer.flush_logs_every_n_steps=10 trainer.val_check_interval=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc94a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel flowtron)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
