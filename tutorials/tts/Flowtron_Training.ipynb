{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7deaeb",
   "metadata": {},
   "source": [
    "# Flowtron Training\n",
    "\n",
    "This notebook covers:\n",
    "1. Generate spectrogram from a pre-trained model on NGC (#TODO)\n",
    "2. Training Flowtron with LJSpeech dataset\n",
    "3. ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9531370",
   "metadata": {},
   "source": [
    "## 1. Generate spectrogram from a pre-trained model on NGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75283b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add a checkpoint to NGC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c95676",
   "metadata": {},
   "source": [
    "## 2. Training Flowtron with LJSpeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ed12f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-31 23:29:22--  https://raw.githubusercontent.com/aroraakshit/NeMo/main/examples/tts/flowtron.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 698 [text/plain]\n",
      "Saving to: ‘flowtron.py’\n",
      "\n",
      "flowtron.py         100%[===================>]     698  --.-KB/s    in 0s      \n",
      "\n",
      "2022-01-31 23:29:22 (32.8 MB/s) - ‘flowtron.py’ saved [698/698]\n",
      "\n",
      "--2022-01-31 23:29:23--  https://raw.githubusercontent.com/aroraakshit/NeMo/main/examples/tts/conf/flowtron.yaml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3498 (3.4K) [text/plain]\n",
      "Saving to: ‘flowtron.yaml’\n",
      "\n",
      "flowtron.yaml       100%[===================>]   3.42K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-01-31 23:29:23 (33.0 MB/s) - ‘flowtron.yaml’ saved [3498/3498]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BRANCH = 'main'\n",
    "\n",
    "!wget https://raw.githubusercontent.com/aroraakshit/NeMo/$BRANCH/examples/tts/flowtron.py\n",
    "!mkdir conf && cd conf && wget https://raw.githubusercontent.com/aroraakshit/NeMo/$BRANCH/examples/tts/conf/flowtron.yaml && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b89ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading LJS dataset\n",
    "# !mkdir data && cd data && wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2 && tar -xvjf archive.tar.bz2 && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6360fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-02-01 00:07:02 experimental:27] Module <function get_argmin_mat at 0x7f0557331820> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-01 00:07:02 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x7f05573318b0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-01 00:07:02 experimental:27] Module <function parse_scale_configs at 0x7f055733c310> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-01 00:07:02 experimental:27] Module <function get_embs_and_timestamps at 0x7f055733c3a0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-01 00:07:04 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-02-01 00:07:04 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-02-01 00:07:04 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:59: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=10)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2022-02-01 00:07:04 exp_manager:283] Experiments will be logged at /akshita/NeMo/tutorials/tts/nemo_experiments/Flowtron/2022-02-01_00-07-04\n",
      "[NeMo I 2022-02-01 00:07:04 exp_manager:648] TensorboardLogger has been set up\n",
      "[NeMo W 2022-02-01 00:07:04 exp_manager:901] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 2 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2022-02-01 00:07:04 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "[NeMo W 2022-02-01 00:07:07 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-02-01 00:07:07 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "Initializing RAdam optimizer\n",
      "\n",
      "  | Name              | Type         | Params\n",
      "---------------------------------------------------\n",
      "0 | speaker_embedding | Embedding    | 128   \n",
      "1 | embedding         | Embedding    | 94.7 K\n",
      "2 | flows             | ModuleList   | 55.4 M\n",
      "3 | encoder           | Encoder      | 5.5 M \n",
      "4 | criterion         | FlowtronLoss | 0     \n",
      "---------------------------------------------------\n",
      "61.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.0 M    Total params\n",
      "243.910   Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s][NeMo W 2022-02-01 00:07:11 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2022-02-01 00:07:14 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2022-02-01 00:07:14 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:407: UserWarning: The number of training samples (16) is smaller than the logging interval Trainer(log_every_n_steps=200). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 0:   0%|                                           | 0/16 [00:00<?, ?it/s][NeMo W 2022-02-01 00:07:15 nemo_logging:349] /akshita/NeMo/nemo/collections/tts/modules/flowtron_submodules.py:729: UserWarning: This overload of addcmul_ is deprecated:\n",
      "    \taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "    Consider using one of the following signatures instead:\n",
      "    \taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "    \n",
      "Epoch 1:  48%|█████▎     | 16/33 [00:09<00:09,  1.72it/s, loss=3.51, v_num=7-04]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  55%|██████     | 18/33 [00:09<00:08,  1.81it/s, loss=3.51, v_num=7-04]\u001b[A\n",
      "Validating:  12%|███▊                            | 2/17 [00:00<00:05,  2.61it/s]\u001b[A\n",
      "Epoch 1:  61%|██████▋    | 20/33 [00:10<00:06,  1.93it/s, loss=3.51, v_num=7-04]\u001b[A\n",
      "Validating:  24%|███████▌                        | 4/17 [00:01<00:03,  3.54it/s]\u001b[A\n",
      "Epoch 1:  67%|███████▎   | 22/33 [00:10<00:05,  2.03it/s, loss=3.51, v_num=7-04]\u001b[A\n",
      "Validating:  35%|███████████▎                    | 6/17 [00:01<00:02,  4.33it/s]\u001b[A\n",
      "Epoch 1:  73%|████████   | 24/33 [00:11<00:04,  2.13it/s, loss=3.51, v_num=7-04]\u001b[A\n",
      "Validating:  47%|███████████████                 | 8/17 [00:02<00:02,  4.28it/s]\u001b[A\n",
      "Epoch 1:  79%|████████▋  | 26/33 [00:11<00:03,  2.22it/s, loss=3.51, v_num=7-04]\u001b[A\n",
      "Validating:  59%|██████████████████▏            | 10/17 [00:02<00:01,  4.42it/s]\u001b[A\n",
      "Epoch 1:  85%|█████████▎ | 28/33 [00:12<00:02,  2.30it/s, loss=3.51, v_num=7-04]\u001b[A\n",
      "Validating:  71%|█████████████████████▉         | 12/17 [00:03<00:01,  4.44it/s]\u001b[A\n",
      "Epoch 1:  91%|██████████ | 30/33 [00:12<00:01,  2.39it/s, loss=3.51, v_num=7-04]\u001b[A\n",
      "Validating:  82%|█████████████████████████▌     | 14/17 [00:03<00:00,  4.55it/s]\u001b[A\n",
      "Epoch 1:  97%|██████████▋| 32/33 [00:13<00:00,  2.45it/s, loss=3.51, v_num=7-04]\u001b[A\n",
      "Validating:  94%|█████████████████████████████▏ | 16/17 [00:03<00:00,  4.52it/s]\u001b[A\n",
      "Epoch 1: 100%|███████████| 33/33 [00:13<00:00,  2.45it/s, loss=3.51, v_num=7-04]\u001b[A\n",
      "                                                                                \u001b[AEpoch 1, global step 31: val_loss reached 1.42352 (best 1.42352), saving model to \"/akshita/NeMo/tutorials/tts/nemo_experiments/Flowtron/2022-02-01_00-07-04/checkpoints/Flowtron--val_loss=1.4235-epoch=1.ckpt\" as top 3\n",
      "Epoch 3:  48%|█████▊      | 16/33 [00:09<00:10,  1.70it/s, loss=1.1, v_num=7-04]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  55%|██████▌     | 18/33 [00:10<00:08,  1.79it/s, loss=1.1, v_num=7-04]\u001b[A\n",
      "Validating:  12%|███▊                            | 2/17 [00:00<00:05,  2.58it/s]\u001b[A\n",
      "Epoch 3:  61%|███████▎    | 20/33 [00:10<00:06,  1.90it/s, loss=1.1, v_num=7-04]\u001b[A\n",
      "Validating:  24%|███████▌                        | 4/17 [00:01<00:03,  3.51it/s]\u001b[A\n",
      "Epoch 3:  67%|████████    | 22/33 [00:10<00:05,  2.01it/s, loss=1.1, v_num=7-04]\u001b[A\n",
      "Validating:  35%|███████████▎                    | 6/17 [00:01<00:02,  4.30it/s]\u001b[A\n",
      "Epoch 3:  73%|████████▋   | 24/33 [00:11<00:04,  2.11it/s, loss=1.1, v_num=7-04]\u001b[A\n",
      "Validating:  47%|███████████████                 | 8/17 [00:02<00:02,  4.27it/s]\u001b[A\n",
      "Epoch 3:  79%|█████████▍  | 26/33 [00:11<00:03,  2.20it/s, loss=1.1, v_num=7-04]\u001b[A\n",
      "Validating:  59%|██████████████████▏            | 10/17 [00:02<00:01,  4.42it/s]\u001b[A\n",
      "Epoch 3:  85%|██████████▏ | 28/33 [00:12<00:02,  2.28it/s, loss=1.1, v_num=7-04]\u001b[A\n",
      "Validating:  71%|█████████████████████▉         | 12/17 [00:03<00:01,  4.42it/s]\u001b[A\n",
      "Epoch 3:  91%|██████████▉ | 30/33 [00:12<00:01,  2.36it/s, loss=1.1, v_num=7-04]\u001b[A\n",
      "Validating:  82%|█████████████████████████▌     | 14/17 [00:03<00:00,  4.52it/s]\u001b[A\n",
      "Epoch 3:  97%|███████████▋| 32/33 [00:13<00:00,  2.43it/s, loss=1.1, v_num=7-04]\u001b[A\n",
      "Validating:  94%|█████████████████████████████▏ | 16/17 [00:03<00:00,  4.50it/s]\u001b[A\n",
      "Epoch 3: 100%|████████████| 33/33 [00:13<00:00,  2.43it/s, loss=1.1, v_num=7-04]\u001b[A\n",
      "                                                                                \u001b[AEpoch 3, global step 63: val_loss reached 1.08132 (best 1.08132), saving model to \"/akshita/NeMo/tutorials/tts/nemo_experiments/Flowtron/2022-02-01_00-07-04/checkpoints/Flowtron--val_loss=1.0813-epoch=3.ckpt\" as top 3\n",
      "Epoch 4: 100%|███████████| 16/16 [00:09<00:00,  1.72it/s, loss=1.08, v_num=7-04]\n",
      "Saving latest checkpoint...\n"
     ]
    }
   ],
   "source": [
    "# sample run to test flowtron\n",
    "\n",
    "!PYTHONPATH=/akshita/NeMo CUDA_VISIBLE_DEVICES=2 HYDRA_FULL_ERROR=1 python flowtron.py train_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_100train_filelist.txt\" validation_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_10val_filelist.txt\" trainer.max_epochs=5 trainer.check_val_every_n_epoch=2 trainer.flush_logs_every_n_steps=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d09d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-02-01 00:09:10 experimental:27] Module <function get_argmin_mat at 0x7fcfd0240700> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-01 00:09:10 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x7fcfd0240790> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-01 00:09:10 experimental:27] Module <function parse_scale_configs at 0x7fcfd024c1f0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-01 00:09:10 experimental:27] Module <function get_embs_and_timestamps at 0x7fcfd024c280> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-02-01 00:09:11 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-02-01 00:09:11 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-02-01 00:09:11 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:59: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=1000)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2022-02-01 00:09:11 exp_manager:283] Experiments will be logged at /akshita/NeMo/tutorials/tts/nemo_experiments/Flowtron/2022-02-01_00-09-11\n",
      "[NeMo I 2022-02-01 00:09:11 exp_manager:648] TensorboardLogger has been set up\n",
      "[NeMo W 2022-02-01 00:09:11 exp_manager:901] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 25 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2022-02-01 00:09:11 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "[NeMo W 2022-02-01 00:09:15 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-02-01 00:09:15 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "Initializing RAdam optimizer\n",
      "\n",
      "  | Name              | Type         | Params\n",
      "---------------------------------------------------\n",
      "0 | speaker_embedding | Embedding    | 128   \n",
      "1 | embedding         | Embedding    | 94.7 K\n",
      "2 | flows             | ModuleList   | 55.4 M\n",
      "3 | encoder           | Encoder      | 5.5 M \n",
      "4 | criterion         | FlowtronLoss | 0     \n",
      "---------------------------------------------------\n",
      "61.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "61.0 M    Total params\n",
      "243.910   Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s][NeMo W 2022-02-01 00:09:19 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2022-02-01 00:09:21 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 0:   0%|                                         | 0/2073 [00:00<?, ?it/s][NeMo W 2022-02-01 00:09:23 nemo_logging:349] /akshita/NeMo/nemo/collections/tts/modules/flowtron_submodules.py:729: UserWarning: This overload of addcmul_ is deprecated:\n",
      "    \taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "    Consider using one of the following signatures instead:\n",
      "    \taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "      exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "    \n",
      "Epoch 0:  33%|██▎    | 694/2073 [06:16<12:28,  1.84it/s, loss=0.259, v_num=9-11]^C\n",
      "[NeMo W 2022-02-01 00:15:39 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "      rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# full run\n",
    "\n",
    "!PYTHONPATH=/akshita/NeMo CUDA_VISIBLE_DEVICES=2 python flowtron.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7144b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
