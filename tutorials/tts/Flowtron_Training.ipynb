{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134a7932",
   "metadata": {},
   "source": [
    "# Flowtron Training\n",
    "\n",
    "This notebook covers:\n",
    "1. Generate spectrogram from a pre-trained model on NGC (#TODO)\n",
    "2. Training Flowtron with LJSpeech dataset\n",
    "3. ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca9a2d",
   "metadata": {},
   "source": [
    "## 1. Generate spectrogram from a pre-trained model on NGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add a checkpoint to NGC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b142462d",
   "metadata": {},
   "source": [
    "## 2. Training Flowtron with LJSpeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26726085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-30 04:58:24--  https://raw.githubusercontent.com/aroraakshit/NeMo/main/examples/tts/flowtron.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 698 [text/plain]\n",
      "Saving to: ‘flowtron.py.1’\n",
      "\n",
      "flowtron.py.1       100%[===================>]     698  --.-KB/s    in 0s      \n",
      "\n",
      "2022-01-30 04:58:24 (18.4 MB/s) - ‘flowtron.py.1’ saved [698/698]\n",
      "\n",
      "--2022-01-30 04:58:25--  https://raw.githubusercontent.com/aroraakshit/NeMo/main/examples/tts/conf/flowtron.yaml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3498 (3.4K) [text/plain]\n",
      "Saving to: ‘flowtron.yaml’\n",
      "\n",
      "flowtron.yaml       100%[===================>]   3.42K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-01-30 04:58:25 (20.8 MB/s) - ‘flowtron.yaml’ saved [3498/3498]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BRANCH = 'main'\n",
    "\n",
    "!wget https://raw.githubusercontent.com/aroraakshit/NeMo/$BRANCH/examples/tts/flowtron.py\n",
    "!mkdir conf && cd conf && wget https://raw.githubusercontent.com/aroraakshit/NeMo/$BRANCH/examples/tts/conf/flowtron.yaml && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3072d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading LJS dataset\n",
    "# !mkdir data && cd data && wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2 && tar -xvjf archive.tar.bz2 && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample run to test flowtron\n",
    "\n",
    "!PYTHONPATH=/akshita/NeMo CUDA_VISIBLE_DEVICES=2 python flowtron.py train_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_100train_filelist.txt\" validation_dataset=\"/akshita/flowtron/filelists/ljs_audiopaths_text_sid_10val_filelist.txt\" trainer.max_epochs=5 trainer.check_val_every_n_epoch=2 trainer.flush_logs_every_n_steps=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498b63bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2022-01-28 09:11:49 experimental:27] Module <function get_argmin_mat at 0x7f9af8d7b700> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 09:11:49 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x7f9af8d7b790> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 09:11:49 experimental:27] Module <function parse_scale_configs at 0x7f9af8d861f0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 09:11:49 experimental:27] Module <function get_embs_and_timestamps at 0x7f9af8d86280> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-28 09:11:51 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-01-28 09:11:51 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "[NeMo W 2022-01-28 09:11:51 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:59: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=1000)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2022-01-28 09:11:51 exp_manager:283] Experiments will be logged at /akshita/NeMo/tutorials/tts/nemo_experiments/Flowtron/2022-01-28_09-11-51\n",
      "[NeMo I 2022-01-28 09:11:51 exp_manager:648] TensorboardLogger has been set up\n",
      "[NeMo W 2022-01-28 09:11:51 exp_manager:901] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 25 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2022-01-28 09:11:51 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:243: LightningDeprecationWarning: `ModelCheckpoint(every_n_val_epochs)` is deprecated in v1.4 and will be removed in v1.6. Please use `every_n_epochs` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "Number of speakers : 1\n",
      "[NeMo W 2022-01-28 09:11:54 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-01-28 09:11:54 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:287: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Initializing RAdam optimizer\n",
      "\n",
      "  | Name              | Type         | Params\n",
      "---------------------------------------------------\n",
      "0 | speaker_embedding | Embedding    | 128   \n",
      "1 | embedding         | Embedding    | 94.7 K\n",
      "2 | flows             | ModuleList   | 27.8 M\n",
      "3 | encoder           | Encoder      | 5.5 M \n",
      "4 | criterion         | FlowtronLoss | 0     \n",
      "---------------------------------------------------\n",
      "33.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "33.5 M    Total params\n",
      "133.831   Total estimated model params size (MB)\n",
      "Validation sanity check: 0it [00:00, ?it/s][NeMo W 2022-01-28 09:11:58 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2022-01-28 09:12:01 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 0:   0%|                                         | 0/2073 [00:00<?, ?it/s]Error executing job with overrides: []\n",
      "Traceback (most recent call last):\n",
      "  File \"flowtron.py\", line 21, in main\n",
      "    trainer.fit(model)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 737, in fit\n",
      "    self._call_and_handle_interrupt(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 682, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 772, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1195, in _run\n",
      "    self._dispatch()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1274, in _dispatch\n",
      "    self.training_type_plugin.start_training(self)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 202, in start_training\n",
      "    self._results = trainer.run_stage()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1284, in run_stage\n",
      "    return self._run_train()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1314, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 234, in advance\n",
      "    self.epoch_loop.run(data_fetcher)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 193, in advance\n",
      "    batch_output = self.batch_loop.run(batch, batch_idx)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 88, in advance\n",
      "    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 215, in advance\n",
      "    result = self._run_optimization(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 266, in _run_optimization\n",
      "    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 378, in _optimizer_step\n",
      "    lightning_module.optimizer_step(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1651, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 164, in step\n",
      "    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 336, in optimizer_step\n",
      "    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 163, in optimizer_step\n",
      "    optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/akshita/NeMo/nemo/collections/tts/modules/flowtron_submodules.py\", line 699, in step\n",
      "    loss = closure()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 148, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 160, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 155, in closure\n",
      "    self._backward_fn(step_output.closure_loss)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 327, in backward_fn\n",
      "    self.trainer.accelerator.backward(loss, optimizer, opt_idx)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 311, in backward\n",
      "    self.precision_plugin.backward(self.lightning_module, closure_loss, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 91, in backward\n",
      "    model.backward(closure_loss, optimizer, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1433, in backward\n",
      "    loss.backward(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.58 GiB (GPU 0; 15.78 GiB total capacity; 10.98 GiB already allocated; 1.08 GiB free; 11.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    }
   ],
   "source": [
    "# full run\n",
    "\n",
    "!PYTHONPATH=/akshita/NeMo python flowtron.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7e3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
